{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFMbyVXorb3B"
      },
      "source": [
        "\n",
        "Complete E-Commerce Recommendation System - Data Processing & API\n",
        "This script processes retail data and provides a REST API for product recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW9fHlHwrNvc"
      },
      "source": [
        "# ===================================================\n",
        "# PART 1: SETUP AND IMPORTS\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UZR9t4i8rGZo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjLC9_lq-Yh0",
        "outputId": "af39e5d3-21d7-409b-e73c-211a0a60e1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All packages imported successfully\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flask flask-cors pyngrok mlxtend openpyxl scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import pickle\n",
        "from datetime import datetime, timezone\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Association Rules\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Web Framework\n",
        "from flask import Flask, request, jsonify, make_response\n",
        "from flask_cors import CORS\n",
        "from functools import lru_cache\n",
        "import json\n",
        "\n",
        "print(\"âœ… All packages imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRGfUGfLrmMc"
      },
      "source": [
        "# ===================================================\n",
        "# PART 2: DATA LOADING AND PREPROCESSING\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jZYH9A4V-fMs"
      },
      "outputs": [],
      "source": [
        "def download_and_load_data():\n",
        "    \"\"\"Download Online Retail dataset from UCI repository\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DOWNLOADING ONLINE RETAIL DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Download dataset\n",
        "    !wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx -O online_retail.xlsx\n",
        "\n",
        "    # Load Excel file\n",
        "    print(\"Loading Online Retail dataset...\")\n",
        "    df = pd.read_excel('online_retail.xlsx')\n",
        "    print(f\"âœ… Loaded {len(df)} transactions\")\n",
        "\n",
        "    # Save as CSV for faster loading\n",
        "    df.to_csv('online_retail_raw.csv', index=False)\n",
        "    print(\"âœ… Saved raw data as CSV\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KxQx81h8-i3s"
      },
      "outputs": [],
      "source": [
        "def preprocess_retail_data(df):\n",
        "    \"\"\"Complete preprocessing pipeline for retail data\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DATA PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 1. Remove missing CustomerID\n",
        "    print(\"\\n1. Handling missing values...\")\n",
        "    initial_shape = df.shape\n",
        "    df = df.dropna(subset=['CustomerID'])\n",
        "    print(f\"   Removed {initial_shape[0] - df.shape[0]} rows with missing CustomerID\")\n",
        "\n",
        "    # 2. Filter positive quantities (remove returns)\n",
        "    df = df[df['Quantity'] > 0]\n",
        "    df = df[df['UnitPrice'] > 0]\n",
        "    print(f\"   Removed negative quantities. Shape: {df.shape}\")\n",
        "\n",
        "    # 3. Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"   Removed duplicates. Shape: {df.shape}\")\n",
        "\n",
        "    # 4. Create TotalPrice feature\n",
        "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "    # 5. Convert date and standardize columns\n",
        "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "    df['CustomerID'] = df['CustomerID'].astype(str).str.split('.').str[0]\n",
        "\n",
        "    # 6. Add time-based features\n",
        "    df['Year'] = df['InvoiceDate'].dt.year\n",
        "    df['Month'] = df['InvoiceDate'].dt.month\n",
        "    df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek\n",
        "    df['Hour'] = df['InvoiceDate'].dt.hour\n",
        "\n",
        "    # 7. Remove outliers using IQR\n",
        "    print(\"\\n2. Removing outliers...\")\n",
        "    Q1 = df['TotalPrice'].quantile(0.25)\n",
        "    Q3 = df['TotalPrice'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df = df[(df['TotalPrice'] >= lower_bound) & (df['TotalPrice'] <= upper_bound)]\n",
        "\n",
        "    print(f\"âœ… Final preprocessed shape: {df.shape}\")\n",
        "\n",
        "    # Save preprocessed data\n",
        "    df.to_csv('online_retail_preprocessed.csv', index=False)\n",
        "    print(\"âœ… Saved preprocessed data\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "132aa63e"
      },
      "outputs": [],
      "source": [
        "def create_customer_features(df):\n",
        "    \"\"\"Create RFM features for customers\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CREATING CUSTOMER FEATURES (RFM)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    max_date = df['InvoiceDate'].max()\n",
        "\n",
        "    customer_features = df.groupby('CustomerID').agg({\n",
        "        'InvoiceDate': lambda x: (max_date - x.max()).days,\n",
        "        'InvoiceNo': 'nunique',\n",
        "        'TotalPrice': 'sum',\n",
        "        'Quantity': 'sum',\n",
        "        'StockCode': 'nunique'\n",
        "    }).reset_index()\n",
        "\n",
        "    customer_features.columns = ['CustomerID', 'Recency', 'Frequency',\n",
        "                                 'Monetary', 'TotalItems', 'UniqueProducts']\n",
        "\n",
        "    customer_features['AvgOrderValue'] = customer_features['Monetary'] / customer_features['Frequency']\n",
        "\n",
        "    print(f\"âœ… Created features for {len(customer_features)} customers\")\n",
        "    customer_features.to_csv('customer_features.csv', index=False)\n",
        "\n",
        "    return customer_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f0Pg1eet-k_5"
      },
      "outputs": [],
      "source": [
        "def create_product_features(df):\n",
        "    \"\"\"Create product-level features\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CREATING PRODUCT FEATURES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    product_features = df.groupby(['StockCode', 'Description']).agg({\n",
        "        'Quantity': 'sum',\n",
        "        'TotalPrice': 'sum',\n",
        "        'CustomerID': 'nunique',\n",
        "        'InvoiceNo': 'nunique',\n",
        "        'UnitPrice': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    product_features.columns = ['StockCode', 'Description', 'TotalQuantity',\n",
        "                                'TotalRevenue', 'UniqueCustomers', 'TotalOrders', 'AvgPrice']\n",
        "\n",
        "    product_features['PopularityScore'] = (\n",
        "        product_features['TotalQuantity'] * 0.3 +\n",
        "        product_features['UniqueCustomers'] * 0.4 +\n",
        "        product_features['TotalOrders'] * 0.3\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Created features for {len(product_features)} products\")\n",
        "    product_features.to_csv('product_features.csv', index=False)\n",
        "\n",
        "    return product_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAEGXTVOryQT"
      },
      "source": [
        "# ===================================================\n",
        "# PART 3: CLUSTERING (K-MEANS)\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "inuaNfsV-nEo"
      },
      "outputs": [],
      "source": [
        "def perform_customer_clustering(customer_features):\n",
        "    \"\"\"Perform customer segmentation using clustering\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CUSTOMER CLUSTERING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    feature_cols = ['Recency', 'Frequency', 'Monetary', 'TotalItems', 'UniqueProducts']\n",
        "    X = customer_features[feature_cols].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "    print(\"\\n1. K-Means Clustering...\")\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
        "    kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "    customer_features['KMeans_Cluster'] = kmeans_labels\n",
        "\n",
        "    cluster_summary = customer_features.groupby('KMeans_Cluster').agg({\n",
        "        'Recency': 'mean',\n",
        "        'Frequency': 'mean',\n",
        "        'Monetary': 'mean'\n",
        "    })\n",
        "\n",
        "    cluster_names = []\n",
        "    for idx, row in cluster_summary.iterrows():\n",
        "        if row['Monetary'] > cluster_summary['Monetary'].median():\n",
        "            if row['Frequency'] > cluster_summary['Frequency'].median():\n",
        "                cluster_names.append('VIP Customers')\n",
        "            else:\n",
        "                cluster_names.append('High Value Customers')\n",
        "        else:\n",
        "            if row['Frequency'] > cluster_summary['Frequency'].median():\n",
        "                cluster_names.append('Loyal Customers')\n",
        "            else:\n",
        "                cluster_names.append('New Customers')\n",
        "\n",
        "    cluster_label_map = dict(zip(cluster_summary.index, cluster_names))\n",
        "    customer_features['Cluster_Label'] = customer_features['KMeans_Cluster'].map(cluster_label_map)\n",
        "\n",
        "    print(f\"âœ… K-Means: Found {len(np.unique(kmeans_labels))} clusters\")\n",
        "\n",
        "    customer_features.to_csv('customer_clusters.csv', index=False)\n",
        "    print(\"âœ… Saved clustering results\")\n",
        "\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    with open('models/kmeans_model.pkl', 'wb') as f:\n",
        "        pickle.dump(kmeans, f)\n",
        "    with open('models/scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    with open('models/pca.pkl', 'wb') as f:\n",
        "        pickle.dump(pca, f)\n",
        "\n",
        "    return customer_features, scaler, pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKD3mIxdKfYV"
      },
      "source": [
        "# ===================================================\n",
        "# PART 4: ASSOCIATION RULES (FP-GROWTH)\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LJWeqoAh-rIU"
      },
      "outputs": [],
      "source": [
        "def generate_association_rules(df):\n",
        "    \"\"\"Generate association rules for market basket analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ASSOCIATION RULE MINING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"Creating transaction matrix...\")\n",
        "    basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().fillna(0)\n",
        "\n",
        "    def encode_units(x):\n",
        "        return 1 if x > 0 else 0\n",
        "\n",
        "    basket_sets = basket.applymap(encode_units)\n",
        "\n",
        "    print(\"Generating frequent itemsets with FP-Growth...\")\n",
        "    frequent_itemsets = fpgrowth(basket_sets, min_support=0.01, use_colnames=True)\n",
        "\n",
        "    print(\"Generating association rules...\")\n",
        "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
        "    rules = rules[rules['confidence'] >= 0.2]\n",
        "    rules = rules.sort_values('lift', ascending=False)\n",
        "\n",
        "    print(f\"âœ… Generated {len(rules)} association rules\")\n",
        "\n",
        "    rules.to_csv('association_rules.csv', index=False)\n",
        "    print(\"âœ… Saved association rules\")\n",
        "\n",
        "    return rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqwH5oAksMlg"
      },
      "source": [
        "# ===================================================\n",
        "# PART 5: CLASSIFICATION MODELS\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e7n3DQ6N-zbv"
      },
      "outputs": [],
      "source": [
        "def train_classification_models(df, customer_features):\n",
        "    \"\"\"Train models to classify transactions as high/low value\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING CLASSIFICATION MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    transaction_features = df.groupby('InvoiceNo').agg({\n",
        "        'TotalPrice': 'sum',\n",
        "        'Quantity': 'sum',\n",
        "        'StockCode': 'nunique',\n",
        "        'CustomerID': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    transaction_features = transaction_features.merge(\n",
        "        customer_features[['CustomerID', 'KMeans_Cluster']],\n",
        "        on='CustomerID',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    median_value = transaction_features['TotalPrice'].median()\n",
        "    transaction_features['Label'] = (transaction_features['TotalPrice'] > median_value).astype(int)\n",
        "\n",
        "    feature_cols = ['Quantity', 'StockCode', 'KMeans_Cluster']\n",
        "    X = transaction_features[feature_cols].fillna(0).values\n",
        "    y = transaction_features['Label'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"\\n1. Training Decision Tree...\")\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_dt = dt_classifier.predict(X_test)\n",
        "    print(f\"   Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "\n",
        "    print(\"\\n2. Training K-Nearest Neighbors...\")\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_knn = knn_classifier.predict(X_test)\n",
        "    print(f\"   Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
        "\n",
        "    with open('models/dt_classifier.pkl', 'wb') as f:\n",
        "        pickle.dump(dt_classifier, f)\n",
        "    with open('models/knn_classifier.pkl', 'wb') as f:\n",
        "        pickle.dump(knn_classifier, f)\n",
        "\n",
        "    print(\"âœ… Models saved successfully\")\n",
        "\n",
        "    return dt_classifier, knn_classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB7fC31DsZSZ"
      },
      "source": [
        "# =================================================\n",
        "# PART 6: OLAP OPERATIONS\n",
        "# ================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "o9huHlnd-4fn"
      },
      "outputs": [],
      "source": [
        "def perform_olap_operations(df):\n",
        "    \"\"\"Perform OLAP operations on the data\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"OLAP OPERATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Slice by country\n",
        "    def slice_by_country(country):\n",
        "        return df[df['Country'] == country]\n",
        "\n",
        "    # Dice by multiple dimensions\n",
        "    def dice_operation(countries, date_range=None):\n",
        "        result = df[df['Country'].isin(countries)]\n",
        "        if date_range:\n",
        "            result = result[(result['InvoiceDate'] >= date_range[0]) &\n",
        "                          (result['InvoiceDate'] <= date_range[1])]\n",
        "        return result\n",
        "\n",
        "    # Roll-up aggregations\n",
        "    country_rollup = df.groupby('Country').agg({\n",
        "        'TotalPrice': 'sum',\n",
        "        'InvoiceNo': 'nunique',\n",
        "        'CustomerID': 'nunique',\n",
        "        'Quantity': 'sum'\n",
        "    }).round(2).sort_values('TotalPrice', ascending=False)\n",
        "\n",
        "    customer_rollup = df.groupby('CustomerID').agg({\n",
        "        'TotalPrice': 'sum',\n",
        "        'InvoiceNo': 'nunique',\n",
        "        'Quantity': 'sum',\n",
        "        'StockCode': 'nunique'\n",
        "    }).round(2).sort_values('TotalPrice', ascending=False)\n",
        "\n",
        "    product_rollup = df.groupby(['StockCode', 'Description']).agg({\n",
        "        'TotalPrice': 'sum',\n",
        "        'Quantity': 'sum',\n",
        "        'CustomerID': 'nunique',\n",
        "        'UnitPrice': 'mean'\n",
        "    }).round(2).sort_values('TotalPrice', ascending=False)\n",
        "\n",
        "    # Save OLAP results\n",
        "    country_rollup.to_csv('olap_country.csv')\n",
        "    customer_rollup.to_csv('olap_customer.csv')\n",
        "    product_rollup.to_csv('olap_product.csv')\n",
        "\n",
        "    print(\"âœ… OLAP operations completed and saved\")\n",
        "\n",
        "    return country_rollup, customer_rollup, product_rollup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syxiHlDFsg1h"
      },
      "source": [
        "# ==================================================\n",
        "# PART 7: MAIN PROCESSING PIPELINE\n",
        "# =================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z77f5If-779",
        "outputId": "ed127bbd-f47a-4a46-ce94-fed44ccb8125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "E-COMMERCE RECOMMENDATION SYSTEM - COMPLETE PIPELINE\n",
            "================================================================================\n",
            "================================================================================\n",
            "DOWNLOADING ONLINE RETAIL DATASET\n",
            "================================================================================\n",
            "Loading Online Retail dataset...\n",
            "âœ… Loaded 541909 transactions\n",
            "âœ… Saved raw data as CSV\n",
            "\n",
            "================================================================================\n",
            "DATA PREPROCESSING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "1. Handling missing values...\n",
            "   Removed 135080 rows with missing CustomerID\n",
            "   Removed negative quantities. Shape: (397884, 8)\n",
            "   Removed duplicates. Shape: (392692, 8)\n",
            "\n",
            "2. Removing outliers...\n",
            "âœ… Final preprocessed shape: (361461, 13)\n",
            "âœ… Saved preprocessed data\n",
            "\n",
            "================================================================================\n",
            "CREATING CUSTOMER FEATURES (RFM)\n",
            "================================================================================\n",
            "âœ… Created features for 4194 customers\n",
            "\n",
            "================================================================================\n",
            "CREATING PRODUCT FEATURES\n",
            "================================================================================\n",
            "âœ… Created features for 3865 products\n",
            "\n",
            "================================================================================\n",
            "CUSTOMER CLUSTERING\n",
            "================================================================================\n",
            "\n",
            "1. K-Means Clustering...\n",
            "âœ… K-Means: Found 4 clusters\n",
            "âœ… Saved clustering results\n",
            "\n",
            "================================================================================\n",
            "ASSOCIATION RULE MINING\n",
            "================================================================================\n",
            "Creating transaction matrix...\n",
            "Generating frequent itemsets with FP-Growth...\n",
            "Generating association rules...\n",
            "âœ… Generated 804 association rules\n",
            "âœ… Saved association rules\n",
            "\n",
            "================================================================================\n",
            "TRAINING CLASSIFICATION MODELS\n",
            "================================================================================\n",
            "\n",
            "1. Training Decision Tree...\n",
            "   Accuracy: 0.8852\n",
            "\n",
            "2. Training K-Nearest Neighbors...\n",
            "   Accuracy: 0.8861\n",
            "âœ… Models saved successfully\n",
            "\n",
            "================================================================================\n",
            "âœ… PIPELINE EXECUTION COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def run_complete_pipeline():\n",
        "    \"\"\"Execute the complete data processing pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"E-COMMERCE RECOMMENDATION SYSTEM - COMPLETE PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 1. Load Data\n",
        "    df = download_and_load_data()\n",
        "\n",
        "    # 2. Preprocess Data\n",
        "    df = preprocess_retail_data(df)\n",
        "\n",
        "    # 3. Create Features\n",
        "    customer_features = create_customer_features(df)\n",
        "    product_features = create_product_features(df)\n",
        "\n",
        "    # 4. Perform Clustering\n",
        "    customer_features, scaler, pca = perform_customer_clustering(customer_features)\n",
        "\n",
        "    # 5. Generate Association Rules\n",
        "    rules = generate_association_rules(df)\n",
        "\n",
        "    # 6. Train Classification Models\n",
        "    dt_model, knn_model = train_classification_models(df, customer_features)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"âœ… PIPELINE EXECUTION COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return {\n",
        "        'df': df,\n",
        "        'customer_features': customer_features,\n",
        "        'product_features': product_features,\n",
        "        'rules': rules,\n",
        "        'models': {\n",
        "            'dt': dt_model,\n",
        "            'knn': knn_model,\n",
        "            'scaler': scaler,\n",
        "            'pca': pca\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run the pipeline\n",
        "pipeline_results = run_complete_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deW20A71stUN"
      },
      "source": [
        "# ===================================================\n",
        "# PART 8: FLASK API WITH NGROK\n",
        "# ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6KEgiHI_EHR"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "# Configure CORS properly for ngrok\n",
        "CORS(app, resources={\n",
        "    r\"/*\": {\n",
        "        \"origins\": \"*\",\n",
        "        \"allow_headers\": [\"Content-Type\", \"Authorization\", \"ngrok-skip-browser-warning\"],\n",
        "        \"methods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"]\n",
        "    }\n",
        "})\n",
        "\n",
        "# Global variables\n",
        "GLOBAL_DATA = {\n",
        "    'df': None,\n",
        "    'customer_features': None,\n",
        "    'product_features': None,\n",
        "    'association_rules': None,\n",
        "    'dt_classifier': None,\n",
        "    'knn_classifier': None\n",
        "}\n",
        "\n",
        "def load_all_data():\n",
        "    \"\"\"Load all processed data and models\"\"\"\n",
        "    global GLOBAL_DATA\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING DATA FOR API\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        GLOBAL_DATA['df'] = pd.read_csv('online_retail_preprocessed.csv')\n",
        "        GLOBAL_DATA['df']['InvoiceDate'] = pd.to_datetime(GLOBAL_DATA['df']['InvoiceDate'])\n",
        "        GLOBAL_DATA['customer_features'] = pd.read_csv('customer_clusters.csv')\n",
        "        GLOBAL_DATA['product_features'] = pd.read_csv('product_features.csv')\n",
        "        GLOBAL_DATA['association_rules'] = pd.read_csv('association_rules.csv')\n",
        "\n",
        "        with open('models/dt_classifier.pkl', 'rb') as f:\n",
        "            GLOBAL_DATA['dt_classifier'] = pickle.load(f)\n",
        "        with open('models/knn_classifier.pkl', 'rb') as f:\n",
        "            GLOBAL_DATA['knn_classifier'] = pickle.load(f)\n",
        "\n",
        "        print(\"âœ… All data and models loaded successfully\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading data: {e}\")\n",
        "        return False\n",
        "\n",
        "# API Routes with proper headers\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    response.headers.add('Access-Control-Allow-Origin', '*')\n",
        "    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,ngrok-skip-browser-warning')\n",
        "    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
        "    return response\n",
        "\n",
        "@app.route('/api/test', methods=['GET', 'OPTIONS'])\n",
        "def test_connection():\n",
        "    \"\"\"Test API connection\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    return jsonify({\n",
        "        'status': 'connected',\n",
        "        'message': 'Connection successful!',\n",
        "        'timestamp': datetime.now(timezone.utc).isoformat()\n",
        "    })\n",
        "\n",
        "@app.route('/api/stats', methods=['GET', 'OPTIONS'])\n",
        "def get_stats():\n",
        "    \"\"\"Get overall statistics\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        df = GLOBAL_DATA['df']\n",
        "        customer_features = GLOBAL_DATA['customer_features']\n",
        "        product_features = GLOBAL_DATA['product_features']\n",
        "\n",
        "        stats = {\n",
        "            'total_transactions': int(len(df)),\n",
        "            'total_customers': int(len(customer_features)),\n",
        "            'total_products': int(len(product_features)),\n",
        "            'total_revenue': float(df['TotalPrice'].sum())\n",
        "        }\n",
        "\n",
        "        return jsonify(stats)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/products', methods=['GET', 'OPTIONS'])\n",
        "def get_products():\n",
        "    \"\"\"Get products with pagination and search\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        page = request.args.get('page', 1, type=int)\n",
        "        per_page = request.args.get('per_page', 20, type=int)\n",
        "        search = request.args.get('search', '')\n",
        "\n",
        "        products = GLOBAL_DATA['product_features'].copy()\n",
        "\n",
        "        if search:\n",
        "            products = products[products['Description'].str.contains(search, case=False, na=False)]\n",
        "\n",
        "        products = products.sort_values('PopularityScore', ascending=False)\n",
        "\n",
        "        total = len(products)\n",
        "        start = (page - 1) * per_page\n",
        "        end = start + per_page\n",
        "\n",
        "        products_page = products.iloc[start:end]\n",
        "\n",
        "        return jsonify({\n",
        "            'products': products_page.to_dict('records'),\n",
        "            'total': total,\n",
        "            'page': page,\n",
        "            'per_page': per_page\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/recommend', methods=['POST', 'OPTIONS'])\n",
        "def get_recommendations():\n",
        "    \"\"\"Get product recommendations\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        data = request.json\n",
        "        stock_code = data.get('stock_code')\n",
        "        n_recommendations = min(data.get('n_recommendations', 10), 20)\n",
        "\n",
        "        df = GLOBAL_DATA['df']\n",
        "\n",
        "        if stock_code:\n",
        "            # Get transactions containing the product\n",
        "            product_transactions = df[df['StockCode'] == stock_code]['InvoiceNo'].unique()\n",
        "\n",
        "            # Get related products from those transactions\n",
        "            related_products = df[df['InvoiceNo'].isin(product_transactions)]\n",
        "            related_products = related_products[related_products['StockCode'] != stock_code]\n",
        "\n",
        "            # Aggregate and rank\n",
        "            top_related = related_products.groupby(['StockCode', 'Description']).agg({\n",
        "                'Quantity': 'sum',\n",
        "                'TotalPrice': 'sum',\n",
        "                'InvoiceNo': 'nunique'\n",
        "            }).nlargest(n_recommendations, 'Quantity')\n",
        "\n",
        "            recommendations = []\n",
        "            for idx, row in top_related.iterrows():\n",
        "                recommendations.append({\n",
        "                    'stock_code': idx[0],\n",
        "                    'description': idx[1],\n",
        "                    'confidence': min(0.9, row['InvoiceNo'] / len(product_transactions)),\n",
        "                    'lift': 2.5\n",
        "                })\n",
        "\n",
        "            return jsonify({\n",
        "                'recommendations': recommendations[:n_recommendations],\n",
        "                'total': len(recommendations)\n",
        "            })\n",
        "\n",
        "        return jsonify({'recommendations': [], 'total': 0})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/customers/<customer_id>', methods=['GET', 'OPTIONS'])\n",
        "def get_customer_profile(customer_id):\n",
        "    \"\"\"Get customer profile (includes recent purchases)\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        customer_features = GLOBAL_DATA['customer_features']\n",
        "        df = GLOBAL_DATA['df']\n",
        "\n",
        "        # Normalize customer id as string (preprocessing stores CustomerID as string)\n",
        "        customer_id_str = str(customer_id)\n",
        "\n",
        "        # Find customer in features (ensure string comparison)\n",
        "        customer_row = customer_features[customer_features['CustomerID'].astype(str) == customer_id_str]\n",
        "\n",
        "        if customer_row.empty:\n",
        "            return jsonify({'error': 'Customer not found'}), 404\n",
        "\n",
        "        profile = customer_row.iloc[0].to_dict()\n",
        "\n",
        "        # Get recent transactions for this customer from the main dataframe\n",
        "        customer_tx = df[df['CustomerID'].astype(str) == customer_id_str].copy()\n",
        "        purchases = []\n",
        "        if not customer_tx.empty:\n",
        "            # Ensure InvoiceDate is datetime and sort by recent\n",
        "            try:\n",
        "                customer_tx['InvoiceDate'] = pd.to_datetime(customer_tx['InvoiceDate'])\n",
        "                customer_tx = customer_tx.sort_values('InvoiceDate', ascending=False)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Select relevant columns and limit rows\n",
        "            cols = []\n",
        "            for c in ['StockCode','Description','Quantity','Country','InvoiceDate','InvoiceNo']:\n",
        "                if c in customer_tx.columns:\n",
        "                    cols.append(c)\n",
        "            if cols:\n",
        "                purchases = customer_tx[cols].head(50).to_dict('records')\n",
        "                # Convert any Timestamp to ISO strings\n",
        "                for p in purchases:\n",
        "                    if 'InvoiceDate' in p and p['InvoiceDate'] is not None:\n",
        "                        try:\n",
        "                            p['InvoiceDate'] = pd.to_datetime(p['InvoiceDate']).isoformat()\n",
        "                        except Exception:\n",
        "                            p['InvoiceDate'] = str(p['InvoiceDate'])\n",
        "\n",
        "        return jsonify({'customer_id': customer_id_str, 'profile': profile, 'purchases': purchases})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/association_rules', methods=['GET', 'OPTIONS'])\n",
        "def get_association_rules():\n",
        "    \"\"\"Get association rules\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        min_confidence = request.args.get('min_confidence', 0.5, type=float)\n",
        "        min_lift = request.args.get('min_lift', 1.0, type=float)\n",
        "\n",
        "        rules = GLOBAL_DATA['association_rules'].copy()\n",
        "\n",
        "        # Filter rules\n",
        "        rules = rules[\n",
        "            (rules['confidence'] >= min_confidence) &\n",
        "            (rules['lift'] >= min_lift)\n",
        "        ]\n",
        "\n",
        "        rules = rules.sort_values('lift', ascending=False)\n",
        "\n",
        "        return jsonify({\n",
        "            'rules': rules.head(50).to_dict('records'),\n",
        "            'total': len(rules)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/olap/slice', methods=['POST', 'OPTIONS'])\n",
        "def olap_slice():\n",
        "    \"\"\"OLAP Slice operation\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        data = request.json\n",
        "        country = data.get('country')\n",
        "\n",
        "        df = GLOBAL_DATA['df']\n",
        "        sliced_data = df[df['Country'] == country]\n",
        "\n",
        "        result = {\n",
        "            'total_transactions': int(len(sliced_data)),\n",
        "            'total_revenue': float(sliced_data['TotalPrice'].sum()),\n",
        "            'unique_customers': int(sliced_data['CustomerID'].nunique()),\n",
        "            'total_quantity': int(sliced_data['Quantity'].sum())\n",
        "        }\n",
        "\n",
        "        return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/olap/dice', methods=['POST', 'OPTIONS'])\n",
        "def olap_dice():\n",
        "    \"\"\"OLAP Dice operation\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        data = request.json\n",
        "        countries = data.get('countries', [])\n",
        "        date_range = data.get('date_range')\n",
        "\n",
        "        df = GLOBAL_DATA['df']\n",
        "        diced_data = df[df['Country'].isin(countries)]\n",
        "\n",
        "        if date_range:\n",
        "            start_date = pd.to_datetime(date_range[0])\n",
        "            end_date = pd.to_datetime(date_range[1])\n",
        "            diced_data = diced_data[\n",
        "                (diced_data['InvoiceDate'] >= start_date) &\n",
        "                (diced_data['InvoiceDate'] <= end_date)\n",
        "            ]\n",
        "\n",
        "        result = {\n",
        "            'total_transactions': int(len(diced_data)),\n",
        "            'total_revenue': float(diced_data['TotalPrice'].sum()),\n",
        "            'unique_customers': int(diced_data['CustomerID'].nunique()),\n",
        "            'total_quantity': int(diced_data['Quantity'].sum())\n",
        "        }\n",
        "\n",
        "        return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/api/olap/rollup', methods=['POST', 'OPTIONS'])\n",
        "def olap_rollup():\n",
        "    \"\"\"OLAP Roll-up operation\"\"\"\n",
        "    if request.method == 'OPTIONS':\n",
        "        return make_response('', 200)\n",
        "\n",
        "    try:\n",
        "        data = request.json\n",
        "        dimension = data.get('dimension', 'country')\n",
        "\n",
        "        df = GLOBAL_DATA['df']\n",
        "\n",
        "        if dimension == 'country':\n",
        "            rollup = df.groupby('Country').agg({\n",
        "                'TotalPrice': 'sum',\n",
        "                'InvoiceNo': 'nunique',\n",
        "                'CustomerID': 'nunique',\n",
        "                'Quantity': 'sum'\n",
        "            }).reset_index()\n",
        "            rollup.columns = ['Country', 'TotalRevenue', 'TotalOrders', 'UniqueCustomers', 'TotalQuantity']\n",
        "\n",
        "        elif dimension == 'customer':\n",
        "            rollup = df.groupby('CustomerID').agg({\n",
        "                'TotalPrice': 'sum',\n",
        "                'InvoiceNo': 'nunique',\n",
        "                'Quantity': 'sum',\n",
        "                'StockCode': 'nunique'\n",
        "            }).reset_index()\n",
        "            rollup.columns = ['CustomerID', 'TotalRevenue', 'TotalOrders', 'TotalQuantity', 'UniqueProducts']\n",
        "\n",
        "        elif dimension == 'product':\n",
        "            rollup = df.groupby(['StockCode', 'Description']).agg({\n",
        "                'TotalPrice': 'sum',\n",
        "                'Quantity': 'sum',\n",
        "                'CustomerID': 'nunique',\n",
        "                'UnitPrice': 'mean'\n",
        "            }).reset_index()\n",
        "            rollup.columns = ['StockCode', 'Description', 'TotalRevenue', 'TotalQuantity', 'UniqueCustomers', 'AvgPrice']\n",
        "\n",
        "        rollup = rollup.sort_values(rollup.columns[2], ascending=False)\n",
        "\n",
        "        return jsonify({\n",
        "            'data': rollup.head(50).to_dict('records'),\n",
        "            'total': len(rollup)\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "4f463f7e",
        "outputId": "697a78b3-c8dd-49f4-870d-d10f24b6340f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample CustomerIDs:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[14066, 15952, 17402, 15068, 13911]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample InvoiceNos:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[550526, 546394, 570210, 551056, 577056]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample StockCodes:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['23005', '22851', '21586', '23340', '79066K']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    customer_features = pd.read_csv('customer_features.csv')\n",
        "    print(\"Sample CustomerIDs:\")\n",
        "    display(customer_features['CustomerID'].sample(5).tolist())\n",
        "except FileNotFoundError:\n",
        "    print(\"customer_features.csv not found. Please run the data processing pipeline first.\")\n",
        "\n",
        "try:\n",
        "    df_preprocessed = pd.read_csv('online_retail_preprocessed.csv')\n",
        "    print(\"\\nSample InvoiceNos:\")\n",
        "    display(df_preprocessed['InvoiceNo'].sample(5).tolist())\n",
        "    print(\"\\nSample StockCodes:\")\n",
        "    display(df_preprocessed['StockCode'].sample(5).tolist())\n",
        "except FileNotFoundError:\n",
        "    print(\"online_retail_preprocessed.csv not found. Please run the data processing pipeline first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrQW8_v0tI9g"
      },
      "source": [
        "# ===============================================\n",
        "# PART 9: NGROK SETUP AND API LAUNCH\n",
        "# ==============================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ay539lFA0Bk",
        "outputId": "904d5d95-bf96-4ee2-e49c-53a8e08497b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "SETTING UP NGROK TUNNEL\n",
            "================================================================================\n",
            "âœ… Running in Google Colab\n",
            "âœ… Ngrok authtoken set from Colab secrets\n",
            "\n",
            "================================================================================\n",
            "LOADING DATA FOR API\n",
            "================================================================================\n",
            "âœ… All data and models loaded successfully\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸš€ E-COMMERCE RECOMMENDATION API IS LIVE!\n",
            "================================================================================\n",
            "\n",
            "ðŸ“± Public URL: NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
            "ðŸŒ Copy this URL to use in the HTML interface\n",
            "\n",
            "ðŸ“ API Endpoints:\n",
            "  â€¢ GET  NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/test - Test connection\n",
            "  â€¢ GET  NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/health - Health check\n",
            "  â€¢ GET  NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/stats - Get statistics\n",
            "  â€¢ GET  NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/products - Get products\n",
            "  â€¢ GET  NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/customers/<id> - Get customer profile\n",
            "  â€¢ POST NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/recommend - Get recommendations\n",
            "  â€¢ POST NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/classify - Classify transaction\n",
            "  â€¢ POST NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/olap/slice - OLAP slice\n",
            "  â€¢ POST NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/olap/dice - OLAP dice\n",
            "  â€¢ POST NgrokTunnel: \"https://blotchier-unlovely-isa.ngrok-free.dev\" -> \"http://localhost:5000\"/api/olap/rollup - OLAP rollup\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âœ… API URL saved to 'api_url.txt'\n",
            "\n",
            "âš ï¸ Keep this cell running to maintain the API connection\n",
            "Press Ctrl+C to stop the server\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 16:59:57] \"OPTIONS /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 16:59:57] \"GET /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 16:59:59] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 16:59:59] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:00] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:00] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:08] \"OPTIONS /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:09] \"GET /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:11] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:11] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:11] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:11] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:37] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:37] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:40] \"OPTIONS /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:41] \"POST /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:43] \"OPTIONS /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:43] \"POST /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:58] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:00:59] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:01:12] \"OPTIONS /api/association_rules?min_confidence=0.5&min_lift=1 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:01:13] \"GET /api/association_rules?min_confidence=0.5&min_lift=1 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:11:52] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:11:53] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:11:56] \"OPTIONS /api/customers/14456 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:11:57] \"\u001b[33mGET /api/customers/14456 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:19:59] \"OPTIONS /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:00] \"GET /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:02] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:02] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:02] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:02] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:04] \"OPTIONS /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:05] \"POST /api/recommend HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:10] \"OPTIONS /api/customers/14445 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:10] \"\u001b[33mGET /api/customers/14445 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:25] \"OPTIONS /api/customers/15952 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:20:25] \"\u001b[33mGET /api/customers/15952 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:42] \"OPTIONS /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:43] \"GET /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:45] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:45] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:45] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:21:45] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:22:00] \"OPTIONS /api/customers/15952 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:22:01] \"\u001b[33mGET /api/customers/15952 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:09] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:10] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:15] \"OPTIONS /api/olap/slice HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:16] \"POST /api/olap/slice HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:20] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:20] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:25] \"OPTIONS /api/olap/rollup HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:25] \"POST /api/olap/rollup HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:38] \"OPTIONS /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:38] \"GET /api/test HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:40] \"OPTIONS /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:40] \"OPTIONS /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:41] \"GET /api/stats HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:41] \"GET /api/products?page=1&per_page=12&search= HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:53] \"OPTIONS /api/customers/14066 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [30/Oct/2025 17:23:53] \"\u001b[33mGET /api/customers/14066 HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "import threading # Import the threading module\n",
        "\n",
        "def setup_ngrok_and_run():\n",
        "    \"\"\"Setup ngrok tunnel and run Flask API\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SETTING UP NGROK TUNNEL\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        IN_COLAB = True\n",
        "        print(\"âœ… Running in Google Colab\")\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"â„¹ï¸ Not running in Colab\")\n",
        "\n",
        "    # Import ngrok\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "    except ImportError:\n",
        "        print(\"Installing pyngrok...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', '-q', 'pyngrok'])\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "    # Setup ngrok authentication\n",
        "    if IN_COLAB:\n",
        "        try:\n",
        "            # Try to get token from Colab secrets\n",
        "            authtoken = userdata.get('NGROK_AUTHTOKEN')\n",
        "            if authtoken:\n",
        "                ngrok.set_auth_token(authtoken)\n",
        "                print(\"âœ… Ngrok authtoken set from Colab secrets\")\n",
        "            else:\n",
        "                # Prompt for token\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"NGROK AUTHENTICATION REQUIRED\")\n",
        "                print(\"=\"*60)\n",
        "                print(\"\\nTo get your ngrok authtoken:\")\n",
        "                print(\"1. Go to: https://dashboard.ngrok.com/signup\")\n",
        "                print(\"2. Sign up for a free account\")\n",
        "                print(\"3. Copy your authtoken from the dashboard\")\n",
        "                print(\"=\"*60)\n",
        "\n",
        "                authtoken = input(\"\\nEnter your ngrok authtoken: \").strip()\n",
        "                if authtoken:\n",
        "                    ngrok.set_auth_token(authtoken)\n",
        "                    print(\"âœ… Ngrok authtoken set successfully\")\n",
        "                else:\n",
        "                    print(\"âš ï¸ No authtoken provided. API will run locally only.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not set ngrok token: {e}\")\n",
        "\n",
        "    # Load data before starting server\n",
        "    if not load_all_data():\n",
        "        print(\"âŒ Failed to load data. Please run the pipeline first.\")\n",
        "        return\n",
        "\n",
        "    # Kill any existing tunnels\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Start Flask in a thread\n",
        "    from threading import Thread\n",
        "\n",
        "    def run_flask():\n",
        "        app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "    flask_thread = Thread(target=run_flask)\n",
        "    flask_thread.daemon = True\n",
        "    flask_thread.start()\n",
        "\n",
        "    # Give Flask time to start\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Create ngrok tunnel\n",
        "    try:\n",
        "        public_url = ngrok.connect(5000, bind_tls=True)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ðŸš€ E-COMMERCE RECOMMENDATION API IS LIVE!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nðŸ“± Public URL: {public_url}\")\n",
        "        print(f\"ðŸŒ Copy this URL to use in the HTML interface\")\n",
        "        print(\"\\nðŸ“ API Endpoints:\")\n",
        "        print(f\"  â€¢ GET  {public_url}/api/test - Test connection\")\n",
        "        print(f\"  â€¢ GET  {public_url}/api/health - Health check\")\n",
        "        print(f\"  â€¢ GET  {public_url}/api/stats - Get statistics\")\n",
        "        print(f\"  â€¢ GET  {public_url}/api/products - Get products\")\n",
        "        print(f\"  â€¢ GET  {public_url}/api/customers/<id> - Get customer profile\")\n",
        "        print(f\"  â€¢ POST {public_url}/api/recommend - Get recommendations\")\n",
        "        print(f\"  â€¢ POST {public_url}/api/classify - Classify transaction\")\n",
        "        print(f\"  â€¢ POST {public_url}/api/olap/slice - OLAP slice\")\n",
        "        print(f\"  â€¢ POST {public_url}/api/olap/dice - OLAP dice\")\n",
        "        print(f\"  â€¢ POST {public_url}/api/olap/rollup - OLAP rollup\")\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "        # Save URL to file\n",
        "        with open('api_url.txt', 'w') as f:\n",
        "            f.write(str(public_url))\n",
        "        print(f\"\\nâœ… API URL saved to 'api_url.txt'\")\n",
        "\n",
        "        # Keep the script running\n",
        "        print(\"\\nâš ï¸ Keep this cell running to maintain the API connection\")\n",
        "        print(\"Press Ctrl+C to stop the server\")\n",
        "\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nðŸ›‘ Shutting down server...\")\n",
        "        ngrok.kill()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error creating ngrok tunnel: {e}\")\n",
        "        print(\"The API is still running locally at http://localhost:5000\")\n",
        "\n",
        "# Start the API\n",
        "setup_ngrok_and_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpQ5Fw6hD3nz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
